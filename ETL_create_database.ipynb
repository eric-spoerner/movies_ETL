{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from config import db_password\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add the clean movie function that takes in the argument, \"movie\".\n",
    "def clean_movie(movie):\n",
    "    \"\"\"\n",
    "    Takes a single wikipedia record, extracts all known values for alternate titles,\n",
    "    and moves them to a list.  Additionally maps redundant/duplicative column names.\n",
    "    \"\"\"\n",
    "    movie = dict(movie) # creates a non-destructive copy\n",
    "    \n",
    "    # Clean alternate titles\n",
    "    alt_titles = dict()\n",
    "    languages = ['Arabic',\n",
    "                 'Cantonese',\n",
    "                 'Chinese',\n",
    "                 'French',\n",
    "                 'Hangul',\n",
    "                 'Hebrew',\n",
    "                 'Hepburn',\n",
    "                 'Japanese',\n",
    "                 'Literally',\n",
    "                 'Mandarin',\n",
    "                 'McCune–Reischauer',\n",
    "                 'Polish',\n",
    "                 'Revised Romanization',\n",
    "                 'Romanized',\n",
    "                 'Russian',\n",
    "                 'Simplified',\n",
    "                 'Traditional',\n",
    "                 'Yiddish']\n",
    "\n",
    "    for language in languages:\n",
    "        if language in movie:\n",
    "            alt_titles[language] = movie[language]\n",
    "            movie.pop(language)\n",
    "\n",
    "    if len(alt_titles) > 0:\n",
    "        movie['alt_titles'] = alt_titles\n",
    "    \n",
    "    def change_column_name(old_name, new_name):\n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)\n",
    "    \n",
    "\n",
    "    change_column_name('Country of origin', 'Country')\n",
    "    change_column_name('Directed by', 'Director(s)')\n",
    "    change_column_name('Director', 'Director(s)')\n",
    "    change_column_name('Distributed by', 'Distributor')\n",
    "    change_column_name('Edited by', 'Editor(s)')\n",
    "    change_column_name('Length', 'Running time')\n",
    "    change_column_name('Produced by', 'Producer(s)')\n",
    "    change_column_name('Producer', 'Producer(s)')\n",
    "    change_column_name('Written by', 'Writer(s)')\n",
    "    change_column_name('Original release', 'Release date')\n",
    "    change_column_name('Productioncompany ', 'Production company(s)')\n",
    "    change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "    change_column_name('Theme music composer', 'Composer(s)')\n",
    "    change_column_name('Music by', 'Composer(s)')\n",
    "    \n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dollars(s):\n",
    "    \"\"\"\n",
    "    Given string s, parse currency strings to float.\n",
    "    \"\"\"\n",
    "    if type(s) != str:\n",
    "        return np.nan\n",
    "    \n",
    "    # form one: r\"\\$\\s*\\d{1,3}\\.?\\d*\\s*[mb]illi?on\"\n",
    "    # form two: r\"\\$\\s*\\d+[,\\.]\\d{3}\"\n",
    "    \n",
    "    # form: \"$###.# billion: \n",
    "    # remove dollar signs, whitespace, and text.  \n",
    "    # Multiply by 1billion\n",
    "    if re.match(r\"\\$\\s*\\d{1,3}\\.?\\d*\\s*billi?on\", s, flags=re.IGNORECASE):\n",
    "        s = re.sub('\\$|\\s|[a-zA-Z]', '', s)\n",
    "        value = float(s) * 10**9\n",
    "        return value\n",
    "     \n",
    "    # form: \"$###.# million: \n",
    "    # remove dollar signs, whitespace, and text.  \n",
    "    # Multiply by 1million   \n",
    "    if re.match(r\"\\$\\s*\\d{1,3}\\.?\\d*\\s*milli?on\", s, flags=re.IGNORECASE):\n",
    "        s = re.sub('\\$|\\s|[a-zA-Z]', '', s)\n",
    "        value = float(s) * 10**6\n",
    "        return value    \n",
    "    \n",
    "    # form: $###,###,###\n",
    "    # strip dollar signs and thousands separators\n",
    "    if re.match(r\"\\$\\s*\\d+[,\\.]\\d{3}\", s, flags=re.IGNORECASE):\n",
    "        s = re.sub('\\$|,|\\.','',s)\n",
    "        value = float(s)\n",
    "        return value\n",
    "        \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_source_files(wiki_file: str,\n",
    "                        kaggle_file: str,\n",
    "                        ratings_file: str):\n",
    "    \"\"\"\n",
    "    Function takes three arguments, each corresponding to the name\n",
    "    of a specific source csv or json file for the three types of data \n",
    "    objects we are importing.  Returns all three objects as unique\n",
    "    pandas DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    kaggle_metadata = pd.read_csv(kaggle_file, low_memory=False)\n",
    "    ratings = pd.read_csv(ratings_file)\n",
    "\n",
    "    with open(wiki_file, mode='r') as file:\n",
    "        wiki_movies_json = json.load(file)\n",
    "    \n",
    "    # Remove TV shows\n",
    "    wiki_movies_json = [wiki_movies_json[i]\\\n",
    "                        for i in range(len(wiki_movies_json))\\\n",
    "                        if 'No. of episodes' not in wiki_movies_json[i]]\n",
    "    \n",
    "    # Iterate through clean movie function to tidy columns\n",
    "    wiki_movies_json = [clean_movie(wiki_movies_json[i]) for i in range(len(wiki_movies_json))]\n",
    "    \n",
    "    # Create dataframe\n",
    "    wiki_movies_df = pd.DataFrame(wiki_movies_json)\n",
    "    \n",
    "    # Extract all IMDB IDs from valid URls and remove records that do not contain them\n",
    "    try:\n",
    "        wiki_movies_df.dropna(subset=['imdb_link'], inplace=True)\n",
    "        wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "        wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f'IMDB extraction failed.  {e}')\n",
    "    \n",
    "    #Consolidate writer columns without overwriting\n",
    "    for col in [\"Writer(s)\", \"Screenplay by\", \"Story by\"]:\n",
    "        wiki_movies_df[col] = wiki_movies_df[col].apply(lambda x: ', '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    wiki_movies_df[\"Writer(s)\"] = wiki_movies_df.apply(lambda row: row[\"Screenplay by\"] if pd.isna(row[\"Writer(s)\"]) else row[\"Writer(s)\"], axis=1)\n",
    "    wiki_movies_df[\"Writer(s)\"] = wiki_movies_df.apply(lambda row: row[\"Story by\"] if pd.isna(row[\"Writer(s)\"]) else row[\"Writer(s)\"], axis=1)\n",
    "\n",
    "    wiki_movies_df.drop(columns=[\"Screenplay by\", \"Story by\"], inplace=True)\n",
    "    \n",
    "    columns_to_drop = [column\\\n",
    "                         for column in wiki_movies_df.columns\\\n",
    "                         if wiki_movies_df[column].count()/len(wiki_movies_df) <= 0.1]   \n",
    "        \n",
    "    wiki_movies_df.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    #Convert year to int\n",
    "    wiki_movies_df[\"year\"] = wiki_movies_df[\"year\"].apply(lambda x: int(x))\n",
    "    \n",
    "    #Regex strings for currency patterns                                   \n",
    "    form_one = r\"\\$\\s*\\d{1,3}\\.?\\d*\\s*[mb]illi?on\"                                       \n",
    "    form_two = r\"\\$\\s*\\d+[,\\.]\\d{3}\"\n",
    "    \n",
    "    # CLEAN BOX OFFICE DATA\n",
    "    box_office = wiki_movies_df['Box office'].dropna()\n",
    "    box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)                                       \n",
    "                                       \n",
    "    wiki_movies_df['box_office'] = box_office.str.\\\n",
    "                                    extract(f\"({form_one}|{form_two})\",\\\n",
    "                                            flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "    \n",
    "    # CLEAN BUDGET data\n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "    budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    # Omit wikipedia citation markers using square brackets\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "\n",
    "    # Remove any hyphens and defer to smaller end of range\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])' , '$', regex=True)\n",
    "\n",
    "    contains_form_one = budget.str.contains(pat=form_one, flags=re.IGNORECASE, na=False)\n",
    "    contains_form_two = budget.str.contains(pat=form_two, flags=re.IGNORECASE, na=False)\n",
    "    \n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "    \n",
    "    # CLEAN RELEASE DATE DATA\n",
    "    release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "    # pattern 1: Month Name, 1-2 digits, 4 digit year\n",
    "    date_pat_1 = r\"\\w*\\s\\d{1,2},\\s\\d{4}\"\n",
    "    matches_pat_1 = release_date.str.contains(date_pat_1, flags=re.IGNORECASE, na=False)\n",
    "\n",
    "    # pattern 2: yyyy-dd-mm\n",
    "    date_pat_2 = r\"\\d{4}[-—–]\\d{2}[-—–]\\d{2}\"\n",
    "    matches_pat_2 = release_date.str.contains(date_pat_2, flags=re.IGNORECASE, na=False)\n",
    "\n",
    "    # pattern 3: (optional day), month name, year\n",
    "    date_pat_3 = r\"\\d{0,2}\\s*\\w{3,10}\\s\\d{4}\"\n",
    "    matches_pat_3 = release_date.str.contains(date_pat_3, flags=re.IGNORECASE, na=False)\n",
    "\n",
    "    # pattern 4: four digit year only\n",
    "    date_pat_4 = r\"\\d{4}\"\n",
    "    matches_pat_4 = release_date.str.contains(date_pat_4, flags=re.IGNORECASE, na=False)\n",
    "\n",
    "    wiki_movies_df['release_date'] = pd.to_datetime(\n",
    "        release_date.str.extract(f'({date_pat_1}|{date_pat_2}|{date_pat_3}|{date_pat_4})')[0],\n",
    "        infer_datetime_format=True,\n",
    "        errors='coerce')\n",
    "    \n",
    "    wiki_movies_df.drop('Release date', axis=1, inplace=True)    \n",
    "    \n",
    "    # RUNTIME DATA\n",
    "    # two string forms transformed: \"# h(ours) ## m(inutes)\", and '### minutes\"\n",
    "    running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    running_time_extract = running_time.str.extract(r\"(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d{1,3})\\s*m\")\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "\n",
    "     # 2. Clean the Kaggle metadata.\n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult', axis='columns')\n",
    "    kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'\n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise') \n",
    "\n",
    "    # 3. Merged the two DataFrames into the movies DataFrame.\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "\n",
    "    # 4. Drop unnecessary columns from the merged DataFrame.\n",
    "    movies_df = movies_df.drop(movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index)\n",
    "    movies_df.drop(columns=['title_wiki', 'release_date_wiki', 'Language', 'Production company(s)'], inplace=True)\n",
    "    \n",
    "    # 5. Add in the function to fill in the missing Kaggle data.\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(\n",
    "         lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "            , axis = 1)\n",
    "        df.drop(columns=wiki_column, inplace=True)                                                           \n",
    "\n",
    "    # 6. Call the function in Step 5 with the DataFrame and columns as the arguments.\n",
    "    fill_missing_kaggle_data(movies_df, kaggle_column='runtime', wiki_column='running_time')\n",
    "    fill_missing_kaggle_data(movies_df, kaggle_column='budget_kaggle', wiki_column='budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, kaggle_column='revenue', wiki_column='box_office')\n",
    "\n",
    "    # 7. Filter the movies DataFrame for specific columns.\n",
    "    movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                           'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                           'genres','original_language','overview','spoken_languages','Country',\n",
    "                           'production_companies','production_countries','Distributor',\n",
    "                           'Producer(s)','Starring','Cinematography','Editor(s)','Writer(s)','Based on'\n",
    "                          ]]\n",
    "\n",
    "    # 8. Rename the columns in the movies DataFrame.\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                      'title_kaggle':'title',\n",
    "                      'url':'wikipedia_url',\n",
    "                      'budget_kaggle':'budget',\n",
    "                      'release_date_kaggle':'release_date',\n",
    "                      'Country':'country',\n",
    "                      'Distributor':'distributor',\n",
    "                      'Producer(s)':'producers',\n",
    "                      'Director':'director',\n",
    "                      'Starring':'starring',\n",
    "                      'Cinematography':'cinematography',\n",
    "                      'Editor(s)':'editors',\n",
    "                      'Writer(s)':'writers',\n",
    "                      'Composer(s)':'composers',\n",
    "                      'Based on':'based_on'\n",
    "                     }, axis='columns', inplace=True)\n",
    "\n",
    "    # 9. Transform and merge the ratings DataFrame\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                        .rename({'userId':'count'}, axis=1) \\\n",
    "                        .pivot(index='movieId', columns='rating', values='count')\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "    \n",
    "    # CONNECTION STRING TEMPLATE: \"postgresql://[user]:[password]@[location]:[port]/[database]\"\n",
    "    db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "    engine = create_engine(db_string)\n",
    "\n",
    "    movies_df.to_sql(name='movies', con=engine, if_exists='replace')\n",
    "    \n",
    "    print(\"Movie data successfully imported to Postgres.\\nCommencing ratings import...\")\n",
    "    \n",
    "    rows_imported = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data in pd.read_csv(f'{ratings_file}', chunksize=1000000):\n",
    "        print(f\"Importing rows {rows_imported} through {rows_imported + len(data)}...\", end='')\n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "        rows_imported += len(data)\n",
    "        print(f'Complete.  {time.time() - start_time} seconds elapsed.')\n",
    "    \n",
    "    \n",
    "    # return wiki_movies_df, movies_df, movies_with_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\espoe\\anaconda3\\envs\\PythonData\\lib\\site-packages\\ipykernel_launcher.py:72: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie data successfully imported to Postgres.\n",
      "Commencing ratings import...\n",
      "Importing rows 0 through 1000000...Complete.  30.29388666152954 seconds elapsed.\n",
      "Importing rows 1000000 through 2000000...Complete.  61.18238401412964 seconds elapsed.\n",
      "Importing rows 2000000 through 3000000...Complete.  94.20128202438354 seconds elapsed.\n",
      "Importing rows 3000000 through 4000000...Complete.  127.75341486930847 seconds elapsed.\n",
      "Importing rows 4000000 through 5000000...Complete.  164.41238117218018 seconds elapsed.\n",
      "Importing rows 5000000 through 6000000...Complete.  197.15625762939453 seconds elapsed.\n",
      "Importing rows 6000000 through 7000000...Complete.  230.86569690704346 seconds elapsed.\n",
      "Importing rows 7000000 through 8000000...Complete.  267.8833820819855 seconds elapsed.\n",
      "Importing rows 8000000 through 9000000...Complete.  300.06341648101807 seconds elapsed.\n",
      "Importing rows 9000000 through 10000000...Complete.  336.57195258140564 seconds elapsed.\n",
      "Importing rows 10000000 through 11000000...Complete.  371.2670593261719 seconds elapsed.\n",
      "Importing rows 11000000 through 12000000...Complete.  405.4039614200592 seconds elapsed.\n",
      "Importing rows 12000000 through 13000000...Complete.  440.86183524131775 seconds elapsed.\n",
      "Importing rows 13000000 through 14000000...Complete.  474.4821357727051 seconds elapsed.\n",
      "Importing rows 14000000 through 15000000...Complete.  508.2918190956116 seconds elapsed.\n",
      "Importing rows 15000000 through 16000000...Complete.  540.0088827610016 seconds elapsed.\n",
      "Importing rows 16000000 through 17000000...Complete.  572.96679854393 seconds elapsed.\n",
      "Importing rows 17000000 through 18000000...Complete.  604.6043963432312 seconds elapsed.\n",
      "Importing rows 18000000 through 19000000...Complete.  636.3443326950073 seconds elapsed.\n",
      "Importing rows 19000000 through 20000000...Complete.  667.5467534065247 seconds elapsed.\n",
      "Importing rows 20000000 through 21000000...Complete.  700.7642722129822 seconds elapsed.\n",
      "Importing rows 21000000 through 22000000...Complete.  733.6490397453308 seconds elapsed.\n",
      "Importing rows 22000000 through 23000000...Complete.  765.4532735347748 seconds elapsed.\n",
      "Importing rows 23000000 through 24000000...Complete.  796.6640110015869 seconds elapsed.\n",
      "Importing rows 24000000 through 25000000...Complete.  828.7992339134216 seconds elapsed.\n",
      "Importing rows 25000000 through 26000000...Complete.  860.9060854911804 seconds elapsed.\n",
      "Importing rows 26000000 through 26024289...Complete.  861.6146326065063 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "file_dir = './data'\n",
    "wiki_file = f'{file_dir}/wikipedia-movies.json'\n",
    "kaggle_file = f'{file_dir}/movies_metadata.csv'\n",
    "ratings_file = f'{file_dir}/ratings.csv'\n",
    "\n",
    "import_source_files(wiki_file=wiki_file, \n",
    "                   kaggle_file=kaggle_file, \n",
    "                   ratings_file=ratings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
